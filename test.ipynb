{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Installing Necessary Libraries\n",
        "## Installation of Required Libraries\n",
        "This cell installs essential libraries such as transformers, datasets, torch, and nltk. These libraries are required for evaluating the NLP model on test set."
      ],
      "metadata": {
        "id": "mB5DZdAqvm8X"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sMUKMlvEaIYm",
        "outputId": "51626ba0-7297-4d17-e04d-a466e2ab65f9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.44.2)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.21.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.4.0+cu121)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.15.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.24.6)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.5.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.4)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.5)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.1.4)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets) (2024.6.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.10.5)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "# Install necessary libraries\n",
        "!pip install transformers datasets torch nltk"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Importing Libraries and Mounting Google Drive\n",
        "This cell imports all the necessary Python libraries required for NLP model evaluation, such as torch, transformers, and nltk. Additionally, it mounts Google Drive to access the trained model."
      ],
      "metadata": {
        "id": "7TwrEKJxv69X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "import torch\n",
        "import json\n",
        "import requests\n",
        "import numpy as np\n",
        "from transformers import BartTokenizer, BartForConditionalGeneration\n",
        "from datasets import Dataset, load_metric\n",
        "from nltk.translate.gleu_score import sentence_gleu"
      ],
      "metadata": {
        "id": "F3KSKDnPaSiY"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ThbVuvCVaVJ9",
        "outputId": "0e689ef7-0682-4c60-a81c-6f4048f0ca05"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "O8uFAB39aZxn"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Downloading the test set\n",
        "## Downloading the Disfl QA test set\n",
        "This cell defines a function to download the test set from the github repository and stores them locally."
      ],
      "metadata": {
        "id": "rXSq6Z0UwPCk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download datasets\n",
        "def download_file(url, filename):\n",
        "    response = requests.get(url)\n",
        "    response.raise_for_status()  # Check for HTTP errors\n",
        "    with open(filename, 'wb') as f:\n",
        "        f.write(response.content)\n",
        "\n",
        "urls = {\n",
        "    \"test\": \"https://raw.githubusercontent.com/google-research-datasets/Disfl-QA/main/test.json\",\n",
        "}\n",
        "\n",
        "for name, url in urls.items():\n",
        "    download_file(url, f\"{name}.json\")\n",
        "\n",
        "print(\"Files downloaded successfully.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2cPxQEvUae_1",
        "outputId": "247d474f-2d01-4d17-f7b4-2bca670e8beb"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files downloaded successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading the test set\n",
        "## Loading Disfl QA test set\n",
        "This cell defines a function to load the JSON test set and extracts required fields (\"original\" and \"disfluent\")."
      ],
      "metadata": {
        "id": "d-X4-QDmwixr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load data\n",
        "def load_disflqa_data(file_path):\n",
        "    with open(file_path, 'r') as f:\n",
        "        data = json.load(f)\n",
        "    return [{\"original\": v[\"original\"], \"disfluent\": v[\"disfluent\"]} for v in data.values()]\n",
        "\n",
        "test_data = load_disflqa_data('test.json')\n",
        "\n",
        "test_dataset = Dataset.from_list(test_data)"
      ],
      "metadata": {
        "id": "kmkSO_1yavnO"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading the best model\n",
        "This cell loads the best \"BART\" model that is already saved for testing."
      ],
      "metadata": {
        "id": "q6MUP9Wtw900"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "1# Define paths\n",
        "#model_path_bart = '/content/drive/MyDrive/DisflQA_Models/BART'\n",
        "model_path_bart = './models/DisflQA_Models/BART'\n",
        "\n",
        "#tokenizer_path_bart = '/content/drive/MyDrive/DisflQA_Tokenizers/BART'\n",
        "tokenizer_path_bart = './models/DisflQA_Tokenizers/BART'\n",
        "\n",
        "# Load tokenizers\n",
        "# Load models and tokenizers\n",
        "models_and_tokenizers = {\n",
        "    \"BART\": (BartForConditionalGeneration.from_pretrained(model_path_bart).to(device), BartTokenizer.from_pretrained(tokenizer_path_bart)),\n",
        "}"
      ],
      "metadata": {
        "id": "jXUMHeZsbFGE"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tokenization Function\n",
        "## Tokenizing Disfl QA Data\n",
        "This cell defines a function for tokenizing the input data. It converts the disfluent questions into the format required by the model and tokenizes both inputs and targets using a specified tokenizer."
      ],
      "metadata": {
        "id": "VsbTsrC2wxik"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for name, (model, tokenizer) in models_and_tokenizers.items():\n",
        "    if tokenizer.pad_token is None:\n",
        "        tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
        "        model.resize_token_embeddings(len(tokenizer))\n",
        "        model.config.pad_token_id = tokenizer.eos_token_id\n",
        "    models_and_tokenizers[name] = (model, tokenizer)"
      ],
      "metadata": {
        "id": "sgPSd_GrbFYP"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Predicting in Batches\n",
        "## Batch Prediction with Trained Model\n",
        "This cell defines a function to generate predictions from the trained\n",
        "\"BART\" model in batches. It processes inputs and decodes the outputs to generate predictions for each input in the test dataset."
      ],
      "metadata": {
        "id": "AvEkpxucw5At"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict in batches\n",
        "def predict_in_batches(model, dataset, tokenizer, batch_size=8):\n",
        "    predictions = []\n",
        "    num_batches = len(dataset) // batch_size + int(len(dataset) % batch_size != 0)\n",
        "    for i in range(num_batches):\n",
        "        try:\n",
        "            # Attempt to select the batch\n",
        "            batch = dataset.select(range(i * batch_size, (i + 1) * batch_size))\n",
        "        except IndexError as e:\n",
        "            # Handle the exception, which might occur on the last batch\n",
        "            print(f\"IndexError: {e} - likely due to the last batch at index {i}.\")\n",
        "            # Handle the last batch case, e.g., selecting only the remaining data\n",
        "            batch = dataset.select(range(i * batch_size, len(dataset)))\n",
        "        except Exception as e:\n",
        "            # Catch any other unexpected exceptions\n",
        "            print(f\"Unexpected error: {e}\")\n",
        "            continue\n",
        "        inputs = tokenizer(batch['disfluent'], return_tensors='pt', padding=True, truncation=True).to(model.device)\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            # Using max_new_tokens to control the length of generated output\n",
        "            outputs = model.generate(**inputs,\n",
        "                                            max_new_tokens=100,\n",
        "                                            #attention_mask=attention_mask,\n",
        "                                            num_beams=8,  # Set the number of beams here\n",
        "                                            #max_length=300,\n",
        "                                            early_stopping=True)\n",
        "\n",
        "            preds = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
        "            predictions.extend(preds)\n",
        "    return predictions"
      ],
      "metadata": {
        "id": "SeUzTzABbJyb"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Computing Evaluation Metrics\n",
        "## Computing Evaluation Metrics (BLEU, GLEU, Accuracy)\n",
        "This cell defines functions to compute various evaluation metrics, including BLEU, GLEU, and accuracy, based on the model predictions and the ground truth labels.\n",
        "### BLEU (Bilingual Evaluation Understudy Score):\n",
        "Measures the precision of n-grams between the generated and reference texts.\n",
        "Higher scores indicate better performance in generating text similar to reference texts.\n",
        "### GLEU (General Language Understanding Evaluation):\n",
        "Evaluates the quality of generated text by comparing it to reference text.\n",
        "Similar to BLEU but considers both precision and recall, focusing on fluency and grammaticality.\n",
        "### Accuracy:\n",
        "Measures the proportion of correct predictions made by the model."
      ],
      "metadata": {
        "id": "_WtmYBTzxfih"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute metrics\n",
        "def compute_gleu(predictions, references):\n",
        "    return np.mean([sentence_gleu([ref.split()], pred.split()) for pred, ref in zip(predictions, references)])\n",
        "\n",
        "def compute_accuracy(predictions, references):\n",
        "    return sum(p == r for p, r in zip(predictions, references)) / len(references)\n",
        "\n",
        "def compute_metrics(predictions, labels):\n",
        "    formatted_preds = [pred.split() for pred in predictions]\n",
        "    formatted_refs = [[ref.split()] for ref in labels]\n",
        "    bleu_metric = load_metric('bleu')\n",
        "    bleu = bleu_metric.compute(predictions=formatted_preds, references=formatted_refs)\n",
        "    gleu = compute_gleu(predictions, labels)\n",
        "    accuracy = compute_accuracy(predictions, labels)\n",
        "    return {\"bleu\": bleu[\"bleu\"], \"gleu\": gleu, \"accuracy\": accuracy}\n",
        "\n",
        "# Evaluate and save predictions\n",
        "predictions = {}\n",
        "for name, models_and_tokenizer in models_and_tokenizers.items():\n",
        "    predictions[name] = predict_in_batches(models_and_tokenizer[0], test_dataset, models_and_tokenizer[1])\n",
        "    labels = test_dataset[\"original\"]\n",
        "    test_results = compute_metrics(predictions[name], labels)\n",
        "    print(f\"{name} Evaluation Results on Test Set: {test_results}\")\n",
        "    with open(f\"{name}_test_results.json\", \"w\") as f:\n",
        "        json.dump(test_results, f)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9lurqEDVbNyQ",
        "outputId": "5466cbf9-234a-4d67-f782-87e7df0dbdc9"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BART Evaluation Results on Test Set: {'bleu': 0.8800623261064311, 'gleu': 0.8724521491785246, 'accuracy': 0.6662091682679111}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The BART model demonstrated strong performance on the test set, with a BLEU score of 0.8801 and a GLEU score of 0.8725, indicating that the generated outputs closely matched the reference answers. The accuracy of 66.62% suggests that the model is reliable in generating correct answers for a significant portion of the test cases. Overall, these results reflect the BART model's effectiveness in handling the task, maintaining high-quality outputs across different evaluation metrics."
      ],
      "metadata": {
        "id": "3vyBswnSx4PU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "random_index = random.randint(0, len(test_dataset) - 1)\n",
        "\n",
        "print(\"Original:\", test_dataset[random_index][\"original\"])\n",
        "print(\"Disfluent:\", test_dataset[random_index][\"disfluent\"])\n",
        "print(\"BART:\", predictions[\"BART\"][random_index])"
      ],
      "metadata": {
        "id": "D4NJz2WldXbe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3f8b4b68-a076-47ed-8ed1-adce310126ed"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original: Who upon arriving gave the original viking settlers a common identity?\n",
            "Disfluent: What or rather who upon arriving gave the original viking settlers a common identity?\n",
            "BART: Who upon arriving gave the original viking settlers a common identity?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hx1v-nmjeUnW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}